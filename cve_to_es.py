#!/usr/bin/env python3

import sys
import json
import os
import elasticsearch
import elasticsearch.helpers
from elasticsearch import Elasticsearch
import glob
import urllib.request as urlreq
import os.path
import gzip
import shutil


if 'ESURL' not in os.environ:
    es_url = "http://localhost:9200"
else:
    es_url = os.environ['ESURL']

diretorio = 'NVD_Feeds/'

es = Elasticsearch([es_url])

class CVE:

    def __init__(self):
        self.ids = []
        self.current = -1

    def add(self, i):
        cve = i['cve']
        cve_id = cve['CVE_data_meta']['ID']
        cve.update(i['impact'])
        cve['year'] = cve_id.split('-')[1]
        cve['just_id'] = cve_id.split('-')[2]

        cve_bulk = {
                    "_op_type": "update",
                    "_index":   "cve-index",
                    "_id":      cve_id,
                    "doc_as_upsert": True,
                    "doc":  cve
                   }

        self.ids.append(cve_bulk)

    def __next__(self):
        "Handle a call to next()"

        self.current = self.current + 1
        if self.current >= len(self.ids):
            raise StopIteration

        return self.ids[self.current]

    def __iter__(self):
        return self

    def __len__(self):
        return len(self.ids)

def getfiles():
    url = 'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2002.json.gz'
    aux = 2002

    #create path if not exist
    if not (os.path.isdir(diretorio)):
        os.mkdir(diretorio)
    
    #download nvd-feeds to repository
    while aux < 2020:
        urlreq.urlretrieve(url, filename= (diretorio + '/' + str(aux) +".json.gz"))
        url = url.replace(str(aux), str(aux+1))
        
        with gzip.open(diretorio + str(aux) + '.json.gz', 'rb') as f_in:
            with open(diretorio + str(aux) + '.json', 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
                os.remove(diretorio + str(aux) + ".json.gz")  
        aux += 1

#read json file and add to cve's object 
def readfiles(jf, cves):
    with open(jf, 'r') as f:
        json_data = json.load(f)
    
    for i in json_data['CVE_Items']:
        cves.add(i)

#bulk cves on ES
def bulk(cves):
    for ok, item in elasticsearch.helpers.streaming_bulk(es, cves, max_retries=2):
        if not ok:
            print("ERROR:")
            print(item)

if __name__ == "__main__":
    getfiles()
    the_cves = CVE()
    v = glob.glob(diretorio + '*.json')
    for item in v:
        readfiles(item, the_cves)      
    bulk(the_cves)
